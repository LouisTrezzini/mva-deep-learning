{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=-1):\n",
    "        self.word2vec = self.load_wordvec(fname, nmax)\n",
    "        self.word2id = {w: i for i, w in enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {i: w for w, i in self.word2id.items()}\n",
    "        self.embeddings = np.array(list(self.word2vec.values()))\n",
    "        \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                # Normalize vectors once and for all\n",
    "                word2vec[word] /= np.linalg.norm(word2vec[word])\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(word2vec)))\n",
    "        \n",
    "        return word2vec\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        \n",
    "        v = self.get_vec(w)\n",
    "        \n",
    "        scores = np.dot(self.embeddings, v)\n",
    "        desc_scores_idx = np.argsort(scores)[::-1]\n",
    "        \n",
    "        return [\n",
    "            self.id2word[i]\n",
    "            for i in desc_scores_idx[1:(K + 1)]\n",
    "        ]\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        v1 = self.get_vec(w1)\n",
    "        v2 = self.get_vec(w2)\n",
    "        return np.dot(v1.T, v2) \n",
    "    \n",
    "    def get_vec(self, w):\n",
    "        \"\"\"\n",
    "        Return embedding associated with word w\n",
    "        \"\"\"\n",
    "        return self.embeddings[self.word2id[w]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "Paris France 0.7058595452409975\n",
      "Germany Berlin 0.7060253015336015\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "['Parisian', 'France', 'paris', 'PARIS', 'Montmartre']\n",
      "['Austria', 'Bavaria', 'Berlin', 'Munich', 'germany']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'))\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'Paris', 'Germany'), ('dog', 'pet', 'cats', 'France', 'Berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                mean = np.mean([\n",
    "                    self.w2v.get_vec(w)\n",
    "                    for w in sent\n",
    "                    if w in self.w2v.word2vec\n",
    "                ], axis=0)\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                mean = np.mean([\n",
    "                    idf[w] * self.w2v.get_vec(w)\n",
    "                    for w in sent\n",
    "                    if w in self.w2v.word2vec\n",
    "                ], axis=0)\n",
    "            \n",
    "            sentemb.append(mean / np.linalg.norm(mean))\n",
    "        \n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)[0]\n",
    "        \n",
    "        scores = np.dot(keys, query)\n",
    "        desc_scores_idx = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # The most similar sentence is the one with second best score (self-similarity is always 1)\n",
    "        most_similar_sent = [\n",
    "            sentences[desc_scores_idx[i]]\n",
    "            for i in range(1, K + 1)\n",
    "        ]\n",
    "        \n",
    "        S = ' '.join(s)\n",
    "        print(f'{K} most similar sentences to \"{S}\":')\n",
    "        for sent in most_similar_sent:\n",
    "            S = ' '.join(sent)\n",
    "            print(f'- \"{S}\"')\n",
    "        \n",
    "        return most_similar_sent\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        vecs = self.encode([s1, s2])\n",
    "        score = np.dot(vecs[0].T, vecs[1])\n",
    "        \n",
    "        S1 = ' '.join(s1)\n",
    "        S2 = ' '.join(s2)\n",
    "        print(f'Score between \"{S1}\" and \"{S2}\": {score}')\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
    "        for w in idf:\n",
    "            idf[w] = np.log10(len(sentences) / idf[w])\n",
    "        \n",
    "        return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "5 most similar sentences to \"1 smiling african american boy .\":\n",
      "- \"an african american man smiling .\"\n",
      "- \"a little african american boy and girl looking up .\"\n",
      "- \"a girl in black hat holding an african american baby .\"\n",
      "- \"an afican american woman standing behind two small african american children .\"\n",
      "- \"african american woman putting braids in an adorable asian girls hair .\"\n",
      "\n",
      "Score between \"1 man singing and 1 man playing a saxophone in a concert .\" and \"10 people venture out to go crosscountry skiing .\": 0.6224503370327624\n",
      "5 most similar sentences to \"1 smiling african american boy .\":\n",
      "- \"an african american man smiling .\"\n",
      "- \"2 african adults and 8 african children looking at pictures on a table .\"\n",
      "- \"an african american man is sitting .\"\n",
      "- \"a girl in black hat holding an african american baby .\"\n",
      "- \"a little african american boy and girl looking up .\"\n",
      "\n",
      "Score between \"1 man singing and 1 man playing a saxophone in a concert .\" and \"10 people venture out to go crosscountry skiing .\": 0.6224503370327624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6224503370327624"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'))\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "with open(os.path.join(PATH_TO_DATA, 'sentences.txt'), 'r') as f:\n",
    "    sentences = [\n",
    "        s.split()\n",
    "        for s in f.readlines()\n",
    "    ]\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar(sentences[10], sentences)  # BoV-mean\n",
    "print()\n",
    "\n",
    "s2v.score(sentences[7], sentences[13])\n",
    "print()\n",
    "\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "s2v.most_similar(sentences[10], sentences, idf)  # BoV-idf\n",
    "print()\n",
    "\n",
    "s2v.score(sentences[7], sentences[13], idf)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "    \n",
    "fasttext_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n",
    "fasttext_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "vocab_fr = set(fasttext_fr.word2vec.keys())\n",
    "vocab_en = set(fasttext_en.word2vec.keys())\n",
    "\n",
    "vocab = sorted(vocab_fr & vocab_en)\n",
    "\n",
    "X = np.vstack([\n",
    "    fasttext_fr.get_vec(word)\n",
    "    for word in vocab\n",
    "]).T\n",
    "\n",
    "Y = np.vstack([\n",
    "    fasttext_en.get_vec(word)\n",
    "    for word in vocab\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "import scipy.linalg\n",
    "\n",
    "U, s, Vh = scipy.linalg.svd(np.dot(Y, X.T))\n",
    "\n",
    "W = np.dot(U, Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mackerel', 'fish', 'shrimp', 'lobster', 'eel']\n",
      "['poissons', 'fish', 'crevettes', 'crustacés', 'truites']\n",
      "['bureau', 'bureaus', 'office', 'offices', 'secretariat']\n",
      "['desk', 'room', 'please', 'placard', 'talk']\n",
      "['truck', 'limousine', 'jeep', 'trucks', 'car']\n",
      "['camionnette', 'camion', 'truck', 'voiture', 'jeep']\n",
      "['salary', 'salaries', 'wages', 'wage', 'payroll']\n",
      "['salaire', 'rémunération', 'salaires', 'rémunérations', 'rémunéré']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "def most_similar_fr_to_en(fr, en, W, w, K=5):\n",
    "    # K most similar words: self.score  -  np.argsort \n",
    "\n",
    "    v_fr = fr.get_vec(w)\n",
    "    v_en = np.dot(W, v_fr)\n",
    "\n",
    "    scores = np.dot(en.embeddings, v_en)\n",
    "    desc_scores_idx = np.argsort(scores)[::-1]\n",
    "\n",
    "    return [\n",
    "        en.id2word[i]\n",
    "        for i in desc_scores_idx[:K]\n",
    "    ]\n",
    "\n",
    "\n",
    "def most_similar_en_to_fr(fr, en, W, w, K=5):\n",
    "    # K most similar words: self.score  -  np.argsort \n",
    "\n",
    "    v_en = en.get_vec(w)\n",
    "    v_fr = np.dot(W.T, v_en)\n",
    "\n",
    "    scores = np.dot(fr.embeddings, v_fr)\n",
    "    desc_scores_idx = np.argsort(scores)[::-1]\n",
    "\n",
    "    return [\n",
    "        fr.id2word[i]\n",
    "        for i in desc_scores_idx[:K]\n",
    "    ]\n",
    "\n",
    "\n",
    "print(most_similar_fr_to_en(fasttext_fr, fasttext_en, W, 'poisson'))\n",
    "print(most_similar_en_to_fr(fasttext_fr, fasttext_en, W, 'fish'))\n",
    "\n",
    "print(most_similar_fr_to_en(fasttext_fr, fasttext_en, W, 'bureau'))\n",
    "print(most_similar_en_to_fr(fasttext_fr, fasttext_en, W, 'desk'))\n",
    "\n",
    "print(most_similar_fr_to_en(fasttext_fr, fasttext_en, W, 'camionnette'))\n",
    "print(most_similar_en_to_fr(fasttext_fr, fasttext_en, W, 'truck'))\n",
    "\n",
    "print(most_similar_fr_to_en(fasttext_fr, fasttext_en, W, 'salaire'))\n",
    "print(most_similar_en_to_fr(fasttext_fr, fasttext_en, W, 'salary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "def load_sst(fname, with_labels=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(fname, 'r') as f:\n",
    "        for s in f.readlines():\n",
    "            if with_labels:\n",
    "                c, sentence = s.split(' ', 1)\n",
    "                y.append(int(c))\n",
    "            else:\n",
    "                sentence = s\n",
    "                \n",
    "            X.append(sentence.split())\n",
    "            \n",
    "    if not with_labels:\n",
    "        return X\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "S_train, y_train = load_sst(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.train'))\n",
    "S_dev, y_dev = load_sst(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.dev'))\n",
    "S_test = load_sst(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.test.X'), with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "X_train = s2v.encode(S_train)\n",
    "X_dev = s2v.encode(S_dev)\n",
    "X_test = s2v.encode(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([[0.27157895, 0.27157895, 0.31964912, 0.37403509, 0.42245614,\n",
      "        0.43789474, 0.42070175, 0.41298246, 0.41017544, 0.41052632],\n",
      "       [0.27186512, 0.27186512, 0.32209343, 0.38426414, 0.44257113,\n",
      "        0.45100105, 0.43273621, 0.42255005, 0.42184756, 0.42219881],\n",
      "       [0.27186512, 0.27186512, 0.32876712, 0.38040042, 0.42079382,\n",
      "        0.43835616, 0.4341412 , 0.42360379, 0.41974008, 0.41938883]]), 1: array([[0.27157895, 0.27157895, 0.31964912, 0.37403509, 0.42245614,\n",
      "        0.43789474, 0.42070175, 0.41298246, 0.41017544, 0.41052632],\n",
      "       [0.27186512, 0.27186512, 0.32209343, 0.38426414, 0.44257113,\n",
      "        0.45100105, 0.43273621, 0.42255005, 0.42184756, 0.42219881],\n",
      "       [0.27186512, 0.27186512, 0.32876712, 0.38040042, 0.42079382,\n",
      "        0.43835616, 0.4341412 , 0.42360379, 0.41974008, 0.41938883]]), 2: array([[0.27157895, 0.27157895, 0.31964912, 0.37403509, 0.42245614,\n",
      "        0.43789474, 0.42070175, 0.41298246, 0.41017544, 0.41052632],\n",
      "       [0.27186512, 0.27186512, 0.32209343, 0.38426414, 0.44257113,\n",
      "        0.45100105, 0.43273621, 0.42255005, 0.42184756, 0.42219881],\n",
      "       [0.27186512, 0.27186512, 0.32876712, 0.38040042, 0.42079382,\n",
      "        0.43835616, 0.4341412 , 0.42360379, 0.41974008, 0.41938883]]), 3: array([[0.27157895, 0.27157895, 0.31964912, 0.37403509, 0.42245614,\n",
      "        0.43789474, 0.42070175, 0.41298246, 0.41017544, 0.41052632],\n",
      "       [0.27186512, 0.27186512, 0.32209343, 0.38426414, 0.44257113,\n",
      "        0.45100105, 0.43273621, 0.42255005, 0.42184756, 0.42219881],\n",
      "       [0.27186512, 0.27186512, 0.32876712, 0.38040042, 0.42079382,\n",
      "        0.43835616, 0.4341412 , 0.42360379, 0.41974008, 0.41938883]]), 4: array([[0.27157895, 0.27157895, 0.31964912, 0.37403509, 0.42245614,\n",
      "        0.43789474, 0.42070175, 0.41298246, 0.41017544, 0.41052632],\n",
      "       [0.27186512, 0.27186512, 0.32209343, 0.38426414, 0.44257113,\n",
      "        0.45100105, 0.43273621, 0.42255005, 0.42184756, 0.42219881],\n",
      "       [0.27186512, 0.27186512, 0.32876712, 0.38040042, 0.42079382,\n",
      "        0.43835616, 0.4341412 , 0.42360379, 0.41974008, 0.41938883]])} [2.7825594 2.7825594 2.7825594 2.7825594 2.7825594]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43778383287920075"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "logreg = LogisticRegressionCV(cv=3, random_state=42, multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(logreg.scores_, logreg.C_)\n",
    "\n",
    "y_pred = logreg.predict(X_dev)\n",
    "\n",
    "accuracy_score(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "with open('logreg_bov_y_test_sst.txt', 'w') as f:\n",
    "    f.writelines([\n",
    "        f'{y}\\n'\n",
    "        for y in y_test_pred\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 1.55695\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 1.5438\n",
      "[3]\tvalid_0's multi_logloss: 1.53077\n",
      "[4]\tvalid_0's multi_logloss: 1.52022\n",
      "[5]\tvalid_0's multi_logloss: 1.51103\n",
      "[6]\tvalid_0's multi_logloss: 1.50336\n",
      "[7]\tvalid_0's multi_logloss: 1.49455\n",
      "[8]\tvalid_0's multi_logloss: 1.48679\n",
      "[9]\tvalid_0's multi_logloss: 1.47991\n",
      "[10]\tvalid_0's multi_logloss: 1.47419\n",
      "[11]\tvalid_0's multi_logloss: 1.46831\n",
      "[12]\tvalid_0's multi_logloss: 1.46239\n",
      "[13]\tvalid_0's multi_logloss: 1.45713\n",
      "[14]\tvalid_0's multi_logloss: 1.45156\n",
      "[15]\tvalid_0's multi_logloss: 1.44676\n",
      "[16]\tvalid_0's multi_logloss: 1.4422\n",
      "[17]\tvalid_0's multi_logloss: 1.43831\n",
      "[18]\tvalid_0's multi_logloss: 1.43274\n",
      "[19]\tvalid_0's multi_logloss: 1.42993\n",
      "[20]\tvalid_0's multi_logloss: 1.42457\n",
      "[21]\tvalid_0's multi_logloss: 1.42011\n",
      "[22]\tvalid_0's multi_logloss: 1.4169\n",
      "[23]\tvalid_0's multi_logloss: 1.41252\n",
      "[24]\tvalid_0's multi_logloss: 1.40888\n",
      "[25]\tvalid_0's multi_logloss: 1.40738\n",
      "[26]\tvalid_0's multi_logloss: 1.40427\n",
      "[27]\tvalid_0's multi_logloss: 1.40148\n",
      "[28]\tvalid_0's multi_logloss: 1.39826\n",
      "[29]\tvalid_0's multi_logloss: 1.39505\n",
      "[30]\tvalid_0's multi_logloss: 1.39318\n",
      "[31]\tvalid_0's multi_logloss: 1.39058\n",
      "[32]\tvalid_0's multi_logloss: 1.38846\n",
      "[33]\tvalid_0's multi_logloss: 1.38572\n",
      "[34]\tvalid_0's multi_logloss: 1.38346\n",
      "[35]\tvalid_0's multi_logloss: 1.38104\n",
      "[36]\tvalid_0's multi_logloss: 1.37852\n",
      "[37]\tvalid_0's multi_logloss: 1.37851\n",
      "[38]\tvalid_0's multi_logloss: 1.37627\n",
      "[39]\tvalid_0's multi_logloss: 1.37514\n",
      "[40]\tvalid_0's multi_logloss: 1.37117\n",
      "[41]\tvalid_0's multi_logloss: 1.37004\n",
      "[42]\tvalid_0's multi_logloss: 1.36821\n",
      "[43]\tvalid_0's multi_logloss: 1.36651\n",
      "[44]\tvalid_0's multi_logloss: 1.36388\n",
      "[45]\tvalid_0's multi_logloss: 1.36279\n",
      "[46]\tvalid_0's multi_logloss: 1.36099\n",
      "[47]\tvalid_0's multi_logloss: 1.36073\n",
      "[48]\tvalid_0's multi_logloss: 1.35868\n",
      "[49]\tvalid_0's multi_logloss: 1.35751\n",
      "[50]\tvalid_0's multi_logloss: 1.35636\n",
      "[51]\tvalid_0's multi_logloss: 1.35451\n",
      "[52]\tvalid_0's multi_logloss: 1.3515\n",
      "[53]\tvalid_0's multi_logloss: 1.35025\n",
      "[54]\tvalid_0's multi_logloss: 1.34869\n",
      "[55]\tvalid_0's multi_logloss: 1.34756\n",
      "[56]\tvalid_0's multi_logloss: 1.34678\n",
      "[57]\tvalid_0's multi_logloss: 1.34576\n",
      "[58]\tvalid_0's multi_logloss: 1.34471\n",
      "[59]\tvalid_0's multi_logloss: 1.34277\n",
      "[60]\tvalid_0's multi_logloss: 1.34189\n",
      "[61]\tvalid_0's multi_logloss: 1.34096\n",
      "[62]\tvalid_0's multi_logloss: 1.34042\n",
      "[63]\tvalid_0's multi_logloss: 1.34057\n",
      "[64]\tvalid_0's multi_logloss: 1.3402\n",
      "[65]\tvalid_0's multi_logloss: 1.34071\n",
      "[66]\tvalid_0's multi_logloss: 1.34009\n",
      "[67]\tvalid_0's multi_logloss: 1.34031\n",
      "[68]\tvalid_0's multi_logloss: 1.33994\n",
      "[69]\tvalid_0's multi_logloss: 1.33938\n",
      "[70]\tvalid_0's multi_logloss: 1.33798\n",
      "[71]\tvalid_0's multi_logloss: 1.3369\n",
      "[72]\tvalid_0's multi_logloss: 1.33508\n",
      "[73]\tvalid_0's multi_logloss: 1.33457\n",
      "[74]\tvalid_0's multi_logloss: 1.33449\n",
      "[75]\tvalid_0's multi_logloss: 1.33483\n",
      "[76]\tvalid_0's multi_logloss: 1.33502\n",
      "[77]\tvalid_0's multi_logloss: 1.33486\n",
      "[78]\tvalid_0's multi_logloss: 1.33479\n",
      "[79]\tvalid_0's multi_logloss: 1.33473\n",
      "[80]\tvalid_0's multi_logloss: 1.33453\n",
      "[81]\tvalid_0's multi_logloss: 1.3347\n",
      "[82]\tvalid_0's multi_logloss: 1.33496\n",
      "[83]\tvalid_0's multi_logloss: 1.33463\n",
      "[84]\tvalid_0's multi_logloss: 1.33419\n",
      "[85]\tvalid_0's multi_logloss: 1.33387\n",
      "[86]\tvalid_0's multi_logloss: 1.33386\n",
      "[87]\tvalid_0's multi_logloss: 1.33462\n",
      "[88]\tvalid_0's multi_logloss: 1.33454\n",
      "[89]\tvalid_0's multi_logloss: 1.33409\n",
      "[90]\tvalid_0's multi_logloss: 1.33371\n",
      "[91]\tvalid_0's multi_logloss: 1.33274\n",
      "[92]\tvalid_0's multi_logloss: 1.33253\n",
      "[93]\tvalid_0's multi_logloss: 1.33179\n",
      "[94]\tvalid_0's multi_logloss: 1.33023\n",
      "[95]\tvalid_0's multi_logloss: 1.33047\n",
      "[96]\tvalid_0's multi_logloss: 1.33006\n",
      "[97]\tvalid_0's multi_logloss: 1.32951\n",
      "[98]\tvalid_0's multi_logloss: 1.32969\n",
      "[99]\tvalid_0's multi_logloss: 1.32955\n",
      "[100]\tvalid_0's multi_logloss: 1.32857\n",
      "[101]\tvalid_0's multi_logloss: 1.32842\n",
      "[102]\tvalid_0's multi_logloss: 1.32872\n",
      "[103]\tvalid_0's multi_logloss: 1.32871\n",
      "[104]\tvalid_0's multi_logloss: 1.32819\n",
      "[105]\tvalid_0's multi_logloss: 1.32793\n",
      "[106]\tvalid_0's multi_logloss: 1.32786\n",
      "[107]\tvalid_0's multi_logloss: 1.32756\n",
      "[108]\tvalid_0's multi_logloss: 1.32606\n",
      "[109]\tvalid_0's multi_logloss: 1.32527\n",
      "[110]\tvalid_0's multi_logloss: 1.32488\n",
      "[111]\tvalid_0's multi_logloss: 1.32512\n",
      "[112]\tvalid_0's multi_logloss: 1.32508\n",
      "[113]\tvalid_0's multi_logloss: 1.32417\n",
      "[114]\tvalid_0's multi_logloss: 1.32382\n",
      "[115]\tvalid_0's multi_logloss: 1.32331\n",
      "[116]\tvalid_0's multi_logloss: 1.32341\n",
      "[117]\tvalid_0's multi_logloss: 1.3236\n",
      "[118]\tvalid_0's multi_logloss: 1.32393\n",
      "[119]\tvalid_0's multi_logloss: 1.32367\n",
      "[120]\tvalid_0's multi_logloss: 1.32377\n",
      "[121]\tvalid_0's multi_logloss: 1.32356\n",
      "[122]\tvalid_0's multi_logloss: 1.32412\n",
      "[123]\tvalid_0's multi_logloss: 1.32374\n",
      "[124]\tvalid_0's multi_logloss: 1.32329\n",
      "[125]\tvalid_0's multi_logloss: 1.32406\n",
      "[126]\tvalid_0's multi_logloss: 1.3237\n",
      "[127]\tvalid_0's multi_logloss: 1.3233\n",
      "[128]\tvalid_0's multi_logloss: 1.32251\n",
      "[129]\tvalid_0's multi_logloss: 1.3218\n",
      "[130]\tvalid_0's multi_logloss: 1.3222\n",
      "[131]\tvalid_0's multi_logloss: 1.32173\n",
      "[132]\tvalid_0's multi_logloss: 1.32088\n",
      "[133]\tvalid_0's multi_logloss: 1.32134\n",
      "[134]\tvalid_0's multi_logloss: 1.32232\n",
      "[135]\tvalid_0's multi_logloss: 1.3218\n",
      "[136]\tvalid_0's multi_logloss: 1.32196\n",
      "[137]\tvalid_0's multi_logloss: 1.32275\n",
      "[138]\tvalid_0's multi_logloss: 1.32234\n",
      "[139]\tvalid_0's multi_logloss: 1.32278\n",
      "[140]\tvalid_0's multi_logloss: 1.32273\n",
      "[141]\tvalid_0's multi_logloss: 1.32265\n",
      "[142]\tvalid_0's multi_logloss: 1.32301\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's multi_logloss: 1.32088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4268846503178928"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "gbm = LGBMClassifier(\n",
    "    n_jobs=8, \n",
    "    n_estimators=500,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train, y_train, verbose=True, eval_set=[(X_dev, y_dev)], early_stopping_rounds=10)\n",
    "\n",
    "y_pred = gbm.predict(X_dev)\n",
    "\n",
    "accuracy_score(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"./data/\"\n",
    "\n",
    "def load_sst(fname, with_labels=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(fname, 'r') as f:\n",
    "        for s in f.readlines():\n",
    "            if with_labels:\n",
    "                c, sentence = s.split(' ', 1)\n",
    "                y.append(int(c))\n",
    "            else:\n",
    "                sentence = s\n",
    "                \n",
    "            X.append(sentence.strip())\n",
    "            \n",
    "    if not with_labels:\n",
    "        return X\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "S_train, y_train = load_sst(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.train'))\n",
    "S_dev, y_dev = load_sst(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.dev'))\n",
    "S_test = load_sst(os.path.join(PATH_TO_DATA, 'SST', 'stsa.fine.test.X'), with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "\n",
    "Si_train = [\n",
    "    keras.preprocessing.text.one_hot(s, 5000)\n",
    "    for s in S_train\n",
    "]\n",
    "Si_dev = [\n",
    "    keras.preprocessing.text.one_hot(s, 5000)\n",
    "    for s in S_dev\n",
    "]\n",
    "Si_test = [\n",
    "    keras.preprocessing.text.one_hot(s, 5000)\n",
    "    for s in S_test\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(Si_train)\n",
    "X_dev = keras.preprocessing.sequence.pad_sequences(Si_dev, maxlen=X_train.shape[1])\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(Si_test, maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 5000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embed_dim),\n",
    "    LSTM(nhid, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(n_classes, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 32)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 185,157\n",
      "Trainable params: 185,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/10\n",
      "8544/8544 [==============================] - 69s 8ms/step - loss: 1.5182 - acc: 0.3228 - val_loss: 1.4171 - val_acc: 0.3688\n",
      "Epoch 2/10\n",
      "8544/8544 [==============================] - 67s 8ms/step - loss: 1.2978 - acc: 0.4377 - val_loss: 1.4623 - val_acc: 0.3506\n",
      "Epoch 3/10\n",
      "8544/8544 [==============================] - 67s 8ms/step - loss: 1.1216 - acc: 0.5345 - val_loss: 1.4967 - val_acc: 0.3406\n",
      "Epoch 4/10\n",
      "8544/8544 [==============================] - 68s 8ms/step - loss: 0.9499 - acc: 0.6179 - val_loss: 1.6477 - val_acc: 0.3370\n",
      "Epoch 5/10\n",
      " 960/8544 [==>...........................] - ETA: 57s - loss: 0.7173 - acc: 0.7500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-6dce2a49c4cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_dev_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/deep/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/deep/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 6\n",
    "n_epochs = 10\n",
    "\n",
    "y_train_hot = keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "y_dev_hot = keras.utils.to_categorical(y_dev, num_classes=n_classes)\n",
    "\n",
    "history = model.fit(X_train, y_train_hot, batch_size=bs, epochs=n_epochs, validation_data=(X_dev, y_dev_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
