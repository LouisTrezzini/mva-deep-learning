\documentclass[11pt]{enpc-article}

\usepackage{mathrsfs}
\usepackage{siunitx}
\usepackage{enumitem}

\title{\vspace{-2em}Deep Learning for Natural Language Processing}
\author{Louis \textsc{Trezzini}}
\date{\today}

\def\lb{\left[}
\def\rb{\right]}
\def\lp{\left(}
\def\rp{\right)}

\usepackage[title]{appendix}
\usepackage{array}

\begin{document}
\maketitle

\setcounter{section}{1}

\section{Multilingual word embeddings}

\subsection*{Question}

We want to minimize $\norm{W X - Y}$ under the constraint that $W \in O_d(\mathbb{R})$.

We notice :
\begin{align*}
  \norm{W X - Y}_F^2
  &= \langle W X - Y, W X - Y \rangle \\
  &= \norm{W X}_F^2 + \norm{Y}_F^2 - 2 \langle W X, Y \rangle \\
  &= \norm{X}_F^2 + \norm{Y}_F^2 - 2 \langle W X, Y \rangle
\end{align*}

So minimizing $\norm{W X - Y}$ is equivalent to maximizing $\langle W X, Y \rangle$.

Let's write $U \Sigma V^T = SVD(Y X^T)$. We have :
\begin{align*}
  \langle W X, Y \rangle
  &= \langle W, Y X^T \rangle \\
  &= \langle W, U \Sigma V^T \rangle \\
  &= \langle U^T W V, \Sigma \rangle
\end{align*}

Let's denote $Q = U^T W V$. We have :
\begin{align*}
  \langle Q, \Sigma \rangle
  &= \text{Trace}(Q^T \Sigma) \\
  &= \sum_{k = 1}^{d} Q_{kk} \Sigma_{kk}
\end{align*}

Because $\Sigma$ is diagonal.

Notice that $Q = U^T W V \in O_d(\mathbb{R})$ because $O_d(\mathbb{R})$ is a group and $U, V, W$ belong to it. It implies that $Q_{kk} \in [-1,1]$.

Thus, this expression is maximized when $Q = I$, since we need to choose $Q_{kk}=1$ for all $k$ because the entries of $\Sigma$ are non-negative.

So :
$$
  Q = I \Leftrightarrow U^T W V = I \Leftrightarrow W = U V^T
$$

\section{Sentence classification with BoV}

\subsection*{Question}

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|}
  \hline
  \textbf{Model}                   & \textbf{Training accuracy} & \textbf{Dev accuracy} \\
  \hline
  Average of word vectors          & \textbf{0.498}             & \textbf{0.441} \\
  Weighted-average of word vectors & 0.483                      & \textbf{0.441} \\
  \hline
  \end{tabular}
  \caption{Performance of logistic regression for sentence classification with BoV}
\end{table}

\section{Deep Learning models for classification}

\subsection*{Question 1}

I used the categorical cross-entropy loss.

Its formula is

$$
  - \frac{1}{N} \sum_{i = 1}^N \sum_{c = 1}^C \mathbf{1}_{\{y_i = c \}} \log(p(\hat{y_i} = c))
$$

where $N$ is the number of examples, $C$ is the number of classes, $y_i$ is the true label of example $i$, and $\hat{y_i}$ is the predicted label of example $i$.

\subsection*{Question 2}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{learning_curve.png}
  \includegraphics[width=0.48\textwidth]{learning_curve_accuracy.png}
  \caption{Evolution of train/dev results w.r.t the number of epochs.}
\end{figure}

We observe that the model overfits a lot. It's not completely surprising, considering there are 185 157 trainable parameters for 8 544 examples.

\subsection*{Question 3}

My architecture looks like this:
% Embedding(
%     input_dim=w2v.embeddings.shape[0],
%     output_dim=w2v.embeddings.shape[1],
%     weights=[w2v.embeddings],
%     trainable=False,
% ),
% Dropout(0.3),
% Conv1D(32, 5, activation='relu'),
% MaxPooling1D(4),
% LSTM(64, dropout=0.2, recurrent_dropout=0.2),
% Dense(n_classes, activation='softmax'),
\begin{enumerate}
  \item A pre-trained Embedding layer, with frozen weights
  \item A 1D convolution with 32 kernels of size 5, followed by a ReLU activation.
  \item A 1D max-pooling layer of size 4.
  \item A LSTM layer with 64 units.
  \item A final dense softmax layer.
\end{enumerate}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.48\textwidth]{learning_curve_innovate.png}
  \includegraphics[width=0.48\textwidth]{learning_curve_accuracy_innovate.png}
  \caption{Evolution of train/dev results w.r.t the number of epochs.}
\end{figure}

As we can see,

\end{document}
